{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41942c82",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e6f0ff;padding:16px;border-radius:8px;border:1px solid #c6ddff;color:#1a1a33;font-family:Arial, sans-serif;line-height:1.6\">\n",
    "  <h1 style=\"margin:0;padding:0;color:#002b80\">Data Preparation</h1>\n",
    "\n",
    "  <p style=\"margin-top:8px\">\n",
    "    This notebook prepares the <strong>welding quality</strong> dataset for <strong>semi-supervised learning</strong> experiments.\n",
    "    The dataset includes several mechanical property targets, each representing a different quality aspect of the weld.\n",
    "  </p>\n",
    "\n",
    "  <p>\n",
    "    Creating one combined target from all properties isnâ€™t feasible, few rows contain <strong>all targets simultaneously</strong>,\n",
    "    which would severely reduce usable data. Instead, we <strong>treat each target independently</strong> and build\n",
    "    a separate prediction task for each property.\n",
    "  </p>\n",
    "\n",
    "  <p>\n",
    "    For every target, the data is split into:\n",
    "  </p>\n",
    "  <ul>\n",
    "    <li><strong>Training set</strong>: Labeled + unlabeled samples (semi-supervised setup)</li>\n",
    "    <li><strong>Validation set</strong>: Labeled samples (for tuning)</li>\n",
    "    <li><strong>Test set</strong>: Labeled samples (for evaluation)</li>\n",
    "  </ul>\n",
    "\n",
    "  <p style=\"margin-top:4px\">\n",
    "    Labeled data is divided <strong>60/20/20</strong> for train/val/test, and all unlabeled samples are added to the training set.\n",
    "    Each target has its own directory with separate <code style=\"color:#004080;background:#f5f9ff;padding:2px 4px;border-radius:4px\">X</code> (features)\n",
    "    and <code style=\"color:#004080;background:#f5f9ff;padding:2px 4px;border-radius:4px\">y</code> (labels) files.\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b791a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import os\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# Style options for plots.\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998).\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bbb051",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "We load the cleaned data localy. If you don't have the csv please run the notebook `01.dataset_cleaning.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b70033f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load the dataset\n",
    "weld_df = pd.read_csv(\"../../data/clean_weld_quality_dataset.csv\")\n",
    "\n",
    "# Shuffle the data to avoid bias when deleting labels\n",
    "weld_df = shuffle(weld_df, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e180a7",
   "metadata": {},
   "source": [
    "Let's take a look at the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7da7ed2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1652 entries, 161 to 1061\n",
      "Data columns (total 55 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   carbon_wt_pct             1652 non-null   float64\n",
      " 1   silicon_wt_pct            1652 non-null   float64\n",
      " 2   manganese_wt_pct          1652 non-null   float64\n",
      " 3   sulfur_wt_pct             1641 non-null   float64\n",
      " 4   phosphorus_wt_pct         1642 non-null   float64\n",
      " 5   nickel_wt_pct             697 non-null    float64\n",
      " 6   chromium_wt_pct           784 non-null    float64\n",
      " 7   molybdenum_wt_pct         791 non-null    float64\n",
      " 8   vanadium_wt_pct           620 non-null    float64\n",
      " 9   copper_wt_pct             564 non-null    float64\n",
      " 10  cobalt_wt_pct             108 non-null    float64\n",
      " 11  tungsten_wt_pct           63 non-null     float64\n",
      " 12  oxygen_ppm                1256 non-null   float64\n",
      " 13  titanium_ppm              865 non-null    float64\n",
      " 14  nitrogen_ppm              1183 non-null   float64\n",
      " 15  aluminium_ppm             502 non-null    float64\n",
      " 16  boron_ppm                 85 non-null     float64\n",
      " 17  niobium_ppm               453 non-null    float64\n",
      " 18  tin_ppm                   291 non-null    float64\n",
      " 19  arsenic_ppm               226 non-null    float64\n",
      " 20  antimony_ppm              254 non-null    float64\n",
      " 21  current_A                 1404 non-null   float64\n",
      " 22  voltage_V                 1404 non-null   float64\n",
      " 23  heat_input_kJmm           1652 non-null   float64\n",
      " 24  interpass_temp_C          1614 non-null   float64\n",
      " 25  pwht_temp_C               1639 non-null   float64\n",
      " 26  pwht_time_h               1639 non-null   float64\n",
      " 27  yield_strength_MPa        780 non-null    float64\n",
      " 28  uts_MPa                   738 non-null    float64\n",
      " 29  elongation_pct            700 non-null    float64\n",
      " 30  reduction_area_pct        705 non-null    float64\n",
      " 31  charpy_temp_C             879 non-null    float64\n",
      " 32  charpy_toughness_J        879 non-null    float64\n",
      " 33  hardness_kgmm2            80 non-null     float64\n",
      " 34  fatt50_C                  31 non-null     float64\n",
      " 35  primary_ferrite_pct       96 non-null     float64\n",
      " 36  ferrite_second_phase_pct  90 non-null     float64\n",
      " 37  acicular_ferrite_pct      90 non-null     float64\n",
      " 38  martensite_pct            89 non-null     float64\n",
      " 39  ferrite_carbide_pct       89 non-null     float64\n",
      " 40  weld_type_FCA             1652 non-null   int64  \n",
      " 41  weld_type_GMAA            1652 non-null   int64  \n",
      " 42  weld_type_GTAA            1652 non-null   int64  \n",
      " 43  weld_type_MMA             1652 non-null   int64  \n",
      " 44  weld_type_NGGMA           1652 non-null   int64  \n",
      " 45  weld_type_NGSAW           1652 non-null   int64  \n",
      " 46  weld_type_SA              1652 non-null   int64  \n",
      " 47  weld_type_SAA             1652 non-null   int64  \n",
      " 48  weld_type_ShMA            1652 non-null   int64  \n",
      " 49  weld_type_TSA             1652 non-null   int64  \n",
      " 50  current_type_AC           1652 non-null   int64  \n",
      " 51  current_type_DC           1652 non-null   int64  \n",
      " 52  electrode_polarity_+      1652 non-null   int64  \n",
      " 53  electrode_polarity_-      1652 non-null   int64  \n",
      " 54  electrode_polarity_0      1652 non-null   int64  \n",
      "dtypes: float64(40), int64(15)\n",
      "memory usage: 722.8 KB\n"
     ]
    }
   ],
   "source": [
    "weld_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a274f511",
   "metadata": {},
   "source": [
    "### Defining target cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fd43dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLS = [\n",
    "    \"yield_strength_MPa\",  # Stress at which plastic deformation begins; measures material strength.\n",
    "    \"uts_MPa\",             # Ultimate tensile strength; maximum stress material can withstand before fracture.\n",
    "    \"elongation_pct\",      # Percent elongation; measure of ductility (total strain before fracture).\n",
    "    \"reduction_area_pct\",  # Percent reduction in cross-sectional area after fracture; another ductility measure.\n",
    "    \"charpy_temp_C\",       # Test temperature for Charpy impact test; defines testing condition.\n",
    "    \"charpy_toughness_J\", ] # Charpy impact energy absorbed; indicates toughness and resistance to brittle fracture.\n",
    "\n",
    "# Potentiel target columns to be dropped from the dataset because too much missing values\n",
    "TODROP = [\n",
    "    \"hardness_kgmm2\",      # Surface hardness; correlates with strength and wear resistance.\n",
    "    \"fatt50_C\"             # 50% Fracture Appearance Transition Temperature; temperature where 50% brittle fracture occurs.\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e10eebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = weld_df.drop(columns=TODROP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b682f7",
   "metadata": {},
   "source": [
    "### Semi-Supervised Train/Val/Test Splits for Each Target\n",
    "\n",
    "For each target, we'll create:\n",
    "- **Train set**: Combination of labeled + unlabeled data (semi-supervised)\n",
    "- **Validation set**: Only labeled data (for tuning)\n",
    "- **Test set**: Only labeled data (for final evaluation)\n",
    "\n",
    "Strategy:\n",
    "1. Split labeled data into train_labeled (60%), val (20%), test (20%)\n",
    "2. Add ALL unlabeled data to training set\n",
    "3. Save X and y separately for each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98de7eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING SEMI-SUPERVISED SPLITS FOR EACH TARGET\n",
      "================================================================================\n",
      "\n",
      "Total samples: 1,652\n",
      "Features: 47\n",
      "Targets: 6\n",
      "\n",
      "================================================================================\n",
      "--> Processing target: yield_strength_MPa\n",
      "================================================================================\n",
      "\n",
      "  Labeled samples:      780 ( 47.2%)\n",
      "  Unlabeled samples:    872 ( 52.8%)\n",
      "\n",
      "  Split sizes:\n",
      "    Train (labeled):        468 samples\n",
      "    Train (unlabeled):      872 samples\n",
      "    Train (total):        1,340 samples (34.9% labeled)\n",
      "    Validation:             156 samples (100% labeled)\n",
      "    Test:                   156 samples (100% labeled)\n",
      "\n",
      "  --> Saved to: ../../data/data_splits\\yield_strength_MPa/\n",
      "    - X_train.csv, y_train.csv (with NaN for unlabeled)\n",
      "    - X_val.csv, y_val.csv\n",
      "    - X_test.csv, y_test.csv\n",
      "\n",
      "================================================================================\n",
      "--> Processing target: uts_MPa\n",
      "================================================================================\n",
      "\n",
      "  Labeled samples:      738 ( 44.7%)\n",
      "  Unlabeled samples:    914 ( 55.3%)\n",
      "\n",
      "  Split sizes:\n",
      "    Train (labeled):        442 samples\n",
      "    Train (unlabeled):      914 samples\n",
      "    Train (total):        1,356 samples (32.6% labeled)\n",
      "    Validation:             148 samples (100% labeled)\n",
      "    Test:                   148 samples (100% labeled)\n",
      "\n",
      "  --> Saved to: ../../data/data_splits\\uts_MPa/\n",
      "    - X_train.csv, y_train.csv (with NaN for unlabeled)\n",
      "    - X_val.csv, y_val.csv\n",
      "    - X_test.csv, y_test.csv\n",
      "\n",
      "================================================================================\n",
      "--> Processing target: elongation_pct\n",
      "================================================================================\n",
      "\n",
      "  Labeled samples:      700 ( 42.4%)\n",
      "  Unlabeled samples:    952 ( 57.6%)\n",
      "\n",
      "  Split sizes:\n",
      "    Train (labeled):        420 samples\n",
      "    Train (unlabeled):      952 samples\n",
      "    Train (total):        1,372 samples (30.6% labeled)\n",
      "    Validation:             140 samples (100% labeled)\n",
      "    Test:                   140 samples (100% labeled)\n",
      "\n",
      "  --> Saved to: ../../data/data_splits\\elongation_pct/\n",
      "    - X_train.csv, y_train.csv (with NaN for unlabeled)\n",
      "    - X_val.csv, y_val.csv\n",
      "    - X_test.csv, y_test.csv\n",
      "\n",
      "================================================================================\n",
      "--> Processing target: reduction_area_pct\n",
      "================================================================================\n",
      "\n",
      "  Labeled samples:      705 ( 42.7%)\n",
      "  Unlabeled samples:    947 ( 57.3%)\n",
      "\n",
      "  Split sizes:\n",
      "    Train (labeled):        423 samples\n",
      "    Train (unlabeled):      947 samples\n",
      "    Train (total):        1,370 samples (30.9% labeled)\n",
      "    Validation:             141 samples (100% labeled)\n",
      "    Test:                   141 samples (100% labeled)\n",
      "\n",
      "  --> Saved to: ../../data/data_splits\\reduction_area_pct/\n",
      "    - X_train.csv, y_train.csv (with NaN for unlabeled)\n",
      "    - X_val.csv, y_val.csv\n",
      "    - X_test.csv, y_test.csv\n",
      "\n",
      "================================================================================\n",
      "--> Processing target: charpy_temp_C\n",
      "================================================================================\n",
      "\n",
      "  Labeled samples:      879 ( 53.2%)\n",
      "  Unlabeled samples:    773 ( 46.8%)\n",
      "\n",
      "  Split sizes:\n",
      "    Train (labeled):        527 samples\n",
      "    Train (unlabeled):      773 samples\n",
      "    Train (total):        1,300 samples (40.5% labeled)\n",
      "    Validation:             176 samples (100% labeled)\n",
      "    Test:                   176 samples (100% labeled)\n",
      "\n",
      "  --> Saved to: ../../data/data_splits\\charpy_temp_C/\n",
      "    - X_train.csv, y_train.csv (with NaN for unlabeled)\n",
      "    - X_val.csv, y_val.csv\n",
      "    - X_test.csv, y_test.csv\n",
      "\n",
      "================================================================================\n",
      "--> Processing target: charpy_toughness_J\n",
      "================================================================================\n",
      "\n",
      "  Labeled samples:      879 ( 53.2%)\n",
      "  Unlabeled samples:    773 ( 46.8%)\n",
      "\n",
      "  Split sizes:\n",
      "    Train (labeled):        527 samples\n",
      "    Train (unlabeled):      773 samples\n",
      "    Train (total):        1,300 samples (40.5% labeled)\n",
      "    Validation:             176 samples (100% labeled)\n",
      "    Test:                   176 samples (100% labeled)\n",
      "\n",
      "  --> Saved to: ../../data/data_splits\\charpy_toughness_J/\n",
      "    - X_train.csv, y_train.csv (with NaN for unlabeled)\n",
      "    - X_val.csv, y_val.csv\n",
      "    - X_test.csv, y_test.csv\n",
      "\n",
      "================================================================================\n",
      "===> ALL SPLITS CREATED SUCCESSFULLY\n",
      "================================================================================\n",
      "\n",
      "Output directory: ../../data/data_splits/\n",
      "  Each target has its own subfolder with 6 files:\n",
      "  - X_train.csv, y_train.csv (semi-supervised)\n",
      "  - X_val.csv, y_val.csv (fully labeled)\n",
      "  - X_test.csv, y_test.csv (fully labeled)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Create directory for splits\n",
    "splits_dir = \"../../data/data_splits\"\n",
    "os.makedirs(splits_dir, exist_ok=True)\n",
    "\n",
    "# Define features (all columns except targets)\n",
    "feature_cols = [col for col in df.columns if col not in TARGET_COLS]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING SEMI-SUPERVISED SPLITS FOR EACH TARGET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal samples: {len(df):,}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Targets: {len(TARGET_COLS)}\")\n",
    "\n",
    "for target in TARGET_COLS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"--> Processing target: {target}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Separate labeled and unlabeled data for this target\n",
    "    labeled_mask = df[target].notna()\n",
    "    unlabeled_mask = df[target].isna()\n",
    "    \n",
    "    df_labeled = df[labeled_mask].copy()\n",
    "    df_unlabeled = df[unlabeled_mask].copy()\n",
    "    \n",
    "    n_labeled = len(df_labeled)\n",
    "    n_unlabeled = len(df_unlabeled)\n",
    "    \n",
    "    print(f\"\\n  Labeled samples:   {n_labeled:>6,} ({n_labeled/len(df)*100:>5.1f}%)\")\n",
    "    print(f\"  Unlabeled samples: {n_unlabeled:>6,} ({n_unlabeled/len(df)*100:>5.1f}%)\")\n",
    "    \n",
    "    # Split labeled data: 60% train, 20% val, 20% test\n",
    "    X_labeled = df_labeled[feature_cols]\n",
    "    y_labeled = df_labeled[[target]]\n",
    "    \n",
    "    # First split: 60% train, 40% temp (val+test)\n",
    "    X_train_labeled, X_temp, y_train_labeled, y_temp = train_test_split(\n",
    "        X_labeled, y_labeled, test_size=0.4, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Second split: split temp into 50% val, 50% test (each 20% of total labeled)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Prepare unlabeled data for training\n",
    "    X_train_unlabeled = df_unlabeled[feature_cols]\n",
    "    y_train_unlabeled = df_unlabeled[[target]]  # Will contain NaN values\n",
    "    \n",
    "    # Combine labeled and unlabeled for training set\n",
    "    X_train = pd.concat([X_train_labeled, X_train_unlabeled], axis=0, ignore_index=True)\n",
    "    y_train = pd.concat([y_train_labeled, y_train_unlabeled], axis=0, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n  Split sizes:\")\n",
    "    print(f\"    Train (labeled):     {len(X_train_labeled):>6,} samples\")\n",
    "    print(f\"    Train (unlabeled):   {len(X_train_unlabeled):>6,} samples\")\n",
    "    print(f\"    Train (total):       {len(X_train):>6,} samples ({len(X_train_labeled)/len(X_train)*100:.1f}% labeled)\")\n",
    "    print(f\"    Validation:          {len(X_val):>6,} samples (100% labeled)\")\n",
    "    print(f\"    Test:                {len(X_test):>6,} samples (100% labeled)\")\n",
    "    \n",
    "    # Create target-specific directory\n",
    "    target_dir = os.path.join(splits_dir, target)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Save splits to CSV\n",
    "    X_train.to_csv(os.path.join(target_dir, \"X_train.csv\"), index=False)\n",
    "    y_train.to_csv(os.path.join(target_dir, \"y_train.csv\"), index=False)\n",
    "    \n",
    "    X_val.to_csv(os.path.join(target_dir, \"X_val.csv\"), index=False)\n",
    "    y_val.to_csv(os.path.join(target_dir, \"y_val.csv\"), index=False)\n",
    "    \n",
    "    X_test.to_csv(os.path.join(target_dir, \"X_test.csv\"), index=False)\n",
    "    y_test.to_csv(os.path.join(target_dir, \"y_test.csv\"), index=False)\n",
    "    \n",
    "    print(f\"\\n  --> Saved to: {target_dir}/\")\n",
    "    print(f\"    - X_train.csv, y_train.csv (with NaN for unlabeled)\")\n",
    "    print(f\"    - X_val.csv, y_val.csv\")\n",
    "    print(f\"    - X_test.csv, y_test.csv\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"===> ALL SPLITS CREATED SUCCESSFULLY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nOutput directory: {splits_dir}/\")\n",
    "print(f\"  Each target has its own subfolder with 6 files:\")\n",
    "print(f\"  - X_train.csv, y_train.csv (semi-supervised)\")\n",
    "print(f\"  - X_val.csv, y_val.csv (fully labeled)\")\n",
    "print(f\"  - X_test.csv, y_test.csv (fully labeled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1452d5e9",
   "metadata": {},
   "source": [
    "### Example: Loading Splits for a Specific Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2446ba16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING SPLITS FOR: yield_strength_MPa\n",
      "================================================================================\n",
      "\n",
      "--> Training set:\n",
      "  X_train shape: (1340, 47)\n",
      "  y_train shape: (1340, 1)\n",
      "  Labeled samples: 468\n",
      "  Unlabeled samples: 872\n",
      "\n",
      "--> Validation set:\n",
      "  X_val shape: (156, 47)\n",
      "  y_val shape: (156, 1)\n",
      "  Missing values: 0\n",
      "\n",
      "--> Test set:\n",
      "  X_test shape: (156, 47)\n",
      "  y_test shape: (156, 1)\n",
      "  Missing values: 0\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Target distribution (labeled training data):\n",
      "count    468.000000\n",
      "mean     507.790812\n",
      "std       93.097066\n",
      "min      315.000000\n",
      "25%      443.000000\n",
      "50%      494.000000\n",
      "75%      554.250000\n",
      "max      920.000000\n",
      "Name: yield_strength_MPa, dtype: float64\n",
      "\n",
      "First few rows of y_train (showing labeled and unlabeled):\n",
      "   yield_strength_MPa\n",
      "0               417.0\n",
      "1               509.0\n",
      "2               427.0\n",
      "3               430.0\n",
      "4               594.0\n",
      "5               555.0\n",
      "6               502.0\n",
      "7               620.0\n",
      "8               546.0\n",
      "9               533.0\n"
     ]
    }
   ],
   "source": [
    "# Example: Load splits for yield_strength_MPa\n",
    "target_name = \"yield_strength_MPa\"\n",
    "target_dir = os.path.join(\"../../data/data_splits\", target_name)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"LOADING SPLITS FOR: {target_name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load training data (semi-supervised)\n",
    "X_train = pd.read_csv(os.path.join(target_dir, \"X_train.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(target_dir, \"y_train.csv\"))\n",
    "\n",
    "# Load validation data (fully labeled)\n",
    "X_val = pd.read_csv(os.path.join(target_dir, \"X_val.csv\"))\n",
    "y_val = pd.read_csv(os.path.join(target_dir, \"y_val.csv\"))\n",
    "\n",
    "# Load test data (fully labeled)\n",
    "X_test = pd.read_csv(os.path.join(target_dir, \"X_test.csv\"))\n",
    "y_test = pd.read_csv(os.path.join(target_dir, \"y_test.csv\"))\n",
    "\n",
    "print(f\"\\n--> Training set:\")\n",
    "print(f\"  X_train shape: {X_train.shape}\")\n",
    "print(f\"  y_train shape: {y_train.shape}\")\n",
    "print(f\"  Labeled samples: {y_train[target_name].notna().sum():,}\")\n",
    "print(f\"  Unlabeled samples: {y_train[target_name].isna().sum():,}\")\n",
    "\n",
    "print(f\"\\n--> Validation set:\")\n",
    "print(f\"  X_val shape: {X_val.shape}\")\n",
    "print(f\"  y_val shape: {y_val.shape}\")\n",
    "print(f\"  Missing values: {y_val[target_name].isna().sum()}\")\n",
    "\n",
    "print(f\"\\n--> Test set:\")\n",
    "print(f\"  X_test shape: {X_test.shape}\")\n",
    "print(f\"  y_test shape: {y_test.shape}\")\n",
    "print(f\"  Missing values: {y_test[target_name].isna().sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Show distribution of target values\n",
    "print(f\"\\nTarget distribution (labeled training data):\")\n",
    "print(y_train[target_name].describe())\n",
    "\n",
    "print(f\"\\nFirst few rows of y_train (showing labeled and unlabeled):\")\n",
    "print(y_train.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
